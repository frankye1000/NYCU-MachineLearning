{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628f01bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from libsvm.svmutil import *\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c468c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "X_train = pd.read_csv(\"data/X_train.csv\", header=None).to_numpy()\n",
    "y_train = pd.read_csv(\"data/y_train.csv\", header=None).to_numpy().reshape(-1)\n",
    "X_test  = pd.read_csv(\"data/X_test.csv\", header=None).to_numpy()\n",
    "y_test  = pd.read_csv(\"data/y_test.csv\", header=None).to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40edaa57",
   "metadata": {},
   "source": [
    "# Part1: \n",
    "Use different kernel functions (linear, polynomial, and RBF kernels) and have comparison between their performance.\n",
    "![svm_param](img/svm_param.PNG)\n",
    "reference from https://github.com/cjlin1/libsvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9bfaa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_type:linear, accuracy: 95.08\n",
      "kernel_type:polynomial, accuracy: 34.68\n",
      "kernel_type:radial basis function, accuracy: 95.32\n"
     ]
    }
   ],
   "source": [
    "kernel_types = {'linear':'-q -t 0',\n",
    "                'polynomial':'-q -t 1',\n",
    "                'radial basis function':'-q -t 2'}\n",
    "\n",
    "for kernel_type in kernel_types:\n",
    "    model = svm_train(y_train, X_train, arg3=kernel_types[kernel_type])\n",
    "    p_labels, p_acc, p_vals = svm_predict(y_test, X_test, model, '-q')\n",
    "    \n",
    "    # p_acc: a tuple including accuracy (for classification), mean-squared error, \n",
    "    # and squared correlation coefficient (for regression).\n",
    "    print(\"kernel_type:{}, accuracy: {:.2f}\".format(kernel_type, p_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a842e49e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5ca6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fa83ca2",
   "metadata": {},
   "source": [
    "# Part2: \n",
    "Please use C-SVC. please do the grid search for finding parameters of the best performing model. For instance, in C-SVC you have a parameter C, and if you use RBF kernel you have another parameter 𝛾, you can search for a set of (C, 𝛾) which gives you best performance in cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47650d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(log2c, log2g, X_train, y_train, X_test ,y_test):\n",
    "    best_lc = log2c[0]\n",
    "    best_lg = log2g[0]\n",
    "    best_acc = 0\n",
    "    for lc in log2c:\n",
    "        for lg in log2g:\n",
    "            arg3 = '-q -t 2 -v 3 -c {} -g {}'.format(2.0**lc, 2.0**lg)\n",
    "            acc = svm_train(y_train, X_train, arg3=arg3)\n",
    "            \n",
    "            \n",
    "            if acc > best_acc:\n",
    "                best_lc = lc\n",
    "                best_lg = lg\n",
    "                best_acc = acc\n",
    "    return best_lc, best_lg, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a2ba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy = 94.18%\n",
      "Cross Validation Accuracy = 41.56%\n",
      "Cross Validation Accuracy = 21.86%\n",
      "Cross Validation Accuracy = 20.32%\n",
      "Cross Validation Accuracy = 78.9%\n",
      "Cross Validation Accuracy = 54%\n",
      "Cross Validation Accuracy = 96.96%\n",
      "Cross Validation Accuracy = 47.66%\n",
      "Cross Validation Accuracy = 21.68%\n",
      "Cross Validation Accuracy = 20.22%\n",
      "Cross Validation Accuracy = 78.78%\n",
      "Cross Validation Accuracy = 54.06%\n",
      "Cross Validation Accuracy = 97.92%\n",
      "Cross Validation Accuracy = 54.34%\n",
      "Cross Validation Accuracy = 25.32%\n",
      "Cross Validation Accuracy = 20.22%\n",
      "Cross Validation Accuracy = 79.04%\n",
      "Cross Validation Accuracy = 40.98%\n",
      "Cross Validation Accuracy = 98.4%\n",
      "Cross Validation Accuracy = 85%\n",
      "Cross Validation Accuracy = 45.7%\n",
      "Cross Validation Accuracy = 25.08%\n",
      "Cross Validation Accuracy = 20.52%\n",
      "Cross Validation Accuracy = 35%\n",
      "Cross Validation Accuracy = 98.54%\n",
      "Cross Validation Accuracy = 85.24%\n",
      "Cross Validation Accuracy = 44.92%\n",
      "Cross Validation Accuracy = 25.24%\n",
      "Cross Validation Accuracy = 20.9%\n",
      "Cross Validation Accuracy = 35.1%\n",
      "Cross Validation Accuracy = 98.4%\n",
      "Cross Validation Accuracy = 85.04%\n",
      "Cross Validation Accuracy = 44.68%\n",
      "Cross Validation Accuracy = 25.42%\n",
      "Cross Validation Accuracy = 26.98%\n",
      "Cross Validation Accuracy = 35.44%\n",
      "Best set (C, gamma)=(2^3, 2^-5), accuracy:98.54%\n"
     ]
    }
   ],
   "source": [
    "log2c = [-5, -3, -1, 1, 3, 5]\n",
    "log2g = [-5, -3, -1, 1, 3, 5]\n",
    "best_lc, best_lg, best_acc = grid_search(log2c, log2g, X_train, y_train, X_test, y_test)\n",
    "print(\"Best set (C, gamma)=(2^{}, 2^{}), accuracy:{}%\".format(best_lc, best_lg, best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49c774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "426a1574",
   "metadata": {},
   "source": [
    "# Part3: \n",
    "Use linear kernel + RBF kernel together (therefore a new kernel function) and compare its performance with respect to others. You would need to find out how to use a user-defined kernel in libsvm.\n",
    "\n",
    "reference from https://stackoverflow.com/questions/7715138/using-precomputed-kernels-with-libsvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a97f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def userDefined_kernel(X, X_, gamma):\n",
    "    kernel_linear = X @ X_.T\n",
    "    kernel_RBF = np.exp(-gamma*cdist(X, X_, 'sqeuclidean'))  # seuclidean：標準化歐式距離\n",
    "    kernel = kernel_linear + kernel_RBF\n",
    "    kernel = np.hstack((np.arange(1, len(X)+1).reshape(-1,1), kernel))\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e0e0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear kernel + RBF kernel accuracy: 95.64%\n"
     ]
    }
   ],
   "source": [
    "K  = userDefined_kernel(X_train, X_train, 2**best_lg)    # best_lg: from part2\n",
    "KK = userDefined_kernel(X_test, X_train, 2**best_lg)     # best_lg: from part2\n",
    "\n",
    "prob  = svm_problem(y_train, K, isKernel=True)\n",
    "param = svm_parameter('-q -t 4')\n",
    "model = svm_train(prob, param)\n",
    "p_label, p_acc, p_vals = svm_predict(y_test, KK, model, '-q')\n",
    "print('linear kernel + RBF kernel accuracy: {:.2f}%'.format(p_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12063f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02723ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecbe3d9d",
   "metadata": {},
   "source": [
    "# Observation\n",
    "C越大，懲罰越大，越少support vectors，越接近hard-margin SVM的概念，卻容易overfitting\n",
    "\n",
    "C越小，懲罰越小，越多support vectors，可以追求更大的margin\n",
    "\n",
    "gamma大，資料點的影響力範圍比較近，對超平面來說，近點的影響力權重較大，容易勾勒出擬合近點的超平面，也容易造成overfitting。\n",
    "\n",
    "gamma小，資料點的影響力範圍比較遠，對超平面來說，較遠的資料點也有影響力，因此能勾勒出平滑、近似直線的超平面。\n",
    "\n",
    "reference from https://rpubs.com/skydome20/R-Note14-SVM-SVR\n",
    "\n",
    "这里面大家需要注意的就是gamma的物理意义，大家提到很多的RBF的幅宽，它会影响每个支持向量对应的高斯的作用范围，从而影响泛化性能。我的理解：如果gamma设的太大，標準差会很小，很小的標準差高斯分布长得又高又瘦， 会造成只会作用于支持向量样本附近，对于未知样本分类效果很差，存在训练准确率可以很高，而测试准确率不高的可能，就是通常说的过训练；而如果设的过小，则会造成平滑效应太大，无法在训练集上得到特别高的准确率，也会影响测试集的准确率。\n",
    "\n",
    "reference from https://blog.csdn.net/lujiandong1/article/details/46386201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369cf390",
   "metadata": {},
   "source": [
    "# Observation\n",
    "嘗試加入polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c50aa4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def userDefined_kernel(X, X_, gamma):\n",
    "    kernel_linear = X @ X_.T\n",
    "    kernel_poly = (1 + gamma*(X @ X_.T))**5\n",
    "    kernel_RBF = np.exp(-gamma*cdist(X, X_, 'sqeuclidean'))  # seuclidean：標準化歐式距離\n",
    "    kernel = kernel_linear + kernel_RBF + kernel_poly\n",
    "    kernel = np.hstack((np.arange(1, len(X)+1).reshape(-1,1), kernel))\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a61dcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear kernel + polynomial kernel +RBF kernel accuracy: 97.88%\n"
     ]
    }
   ],
   "source": [
    "K  = userDefined_kernel(X_train, X_train, 2**best_lg)    # best_lg: from part2\n",
    "KK = userDefined_kernel(X_test, X_train, 2**best_lg)     # best_lg: from part2\n",
    "\n",
    "prob  = svm_problem(y_train, K, isKernel=True)\n",
    "param = svm_parameter('-q -t 4')\n",
    "model = svm_train(prob, param)\n",
    "p_label, p_acc, p_vals = svm_predict(y_test, KK, model, '-q')\n",
    "print('linear kernel + polynomial kernel +RBF kernel accuracy: {:.2f}%'.format(p_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c22f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
